{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a04eecd-56da-4a9a-b940-3818c609daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6140966-d182-4645-8fa0-353923d4c0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:   0%|                                   | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CUSIP: 012432DW\n",
      "Clicked 'Accept' button\n",
      "Performed dummy click\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  10%|██▋                        | 1/10 [00:04<00:40,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 48 PDFs for 012432DW\n",
      "\n",
      "Processing CUSIP: 65818PNN\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  20%|█████▍                     | 2/10 [00:07<00:30,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 21 PDFs for 65818PNN\n",
      "\n",
      "Processing CUSIP: 469205LF\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  30%|████████                   | 3/10 [00:11<00:27,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 23 PDFs for 469205LF\n",
      "\n",
      "Processing CUSIP: 469480AT\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  40%|██████████▊                | 4/10 [00:15<00:21,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 15 PDFs for 469480AT\n",
      "\n",
      "Processing CUSIP: 490728G2\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  50%|█████████████▌             | 5/10 [00:18<00:17,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 5 PDFs for 490728G2\n",
      "\n",
      "Processing CUSIP: 596564RX\n",
      "Attempt 1: Stale tab. Retrying...\n",
      "Filter click failed: Message: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  60%|████████████████▏          | 6/10 [00:31<00:26,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDFs found for 596564RX\n",
      "\n",
      "Processing CUSIP: 646066UM\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  70%|██████████████████▉        | 7/10 [00:44<00:26,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter click failed: Message: \n",
      "\n",
      "Collected 2 PDFs for 646066UM\n",
      "\n",
      "Processing CUSIP: 646066UW\n",
      "Attempt 1: Stale tab. Retrying...\n",
      "Filter click failed: Message: \n",
      "\n",
      "Collected 2 PDFs for 646066UW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  80%|█████████████████████▌     | 8/10 [00:57<00:20, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CUSIP: 91428LJN\n",
      "Attempt 1: Stale tab. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping CUSIPs:  90%|████████████████████████▎  | 9/10 [01:10<00:10, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter click failed: Message: \n",
      "\n",
      "No PDFs found for 91428LJN\n",
      "\n",
      "Processing CUSIP: 20775DXL\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- CONFIG -------------------------\n",
    "TEST_MODE = True\n",
    "EXCEL_PATH = \"Higher Ed cusips_test.xlsx\" if TEST_MODE else \"Higher Ed cusips.xlsx\"\n",
    "EXCEL_OUTPUT = \"disclosure_document_list_test.csv\" if TEST_MODE else \"disclosure_document_list.csv\"\n",
    "ROOT_DIR = Path(\"university_pdfs_test\" if TEST_MODE else \"university_pdfs\")\n",
    "FAILED_LOG_PATH = \"failed_downloads_test.csv\" if TEST_MODE else \"failed_downloads.csv\"\n",
    "TMP_DIR = Path(\"__tmp_downloads\")\n",
    "WAIT_TIME = 10\n",
    "TIMEOUT = 20\n",
    "SLEEP = 0.3\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Load CUSIP data\n",
    "holdings = pd.read_excel(EXCEL_PATH, sheet_name='Holdings')\n",
    "index = pd.read_excel(EXCEL_PATH, sheet_name='Index')\n",
    "df_cusips = pd.concat([holdings[['Cusip 8', 'CREDIT']], index[['Cusip 8', 'CREDIT']]]).reset_index(drop=True)\n",
    "df_cusips = df_cusips.groupby('CREDIT')['Cusip 8'].first().reset_index()\n",
    "list_cusip = df_cusips['Cusip 8'].to_list()\n",
    "\n",
    "def slugify(text):\n",
    "    return re.sub(r\"[^\\w\\-. ]\", \"_\", text).strip().replace(\" \", \"_\")\n",
    "\n",
    "def setup_browser(download_dir):\n",
    "    chrome_opts = Options()\n",
    "    chrome_opts.add_argument(\"--no-sandbox\")\n",
    "    chrome_opts.add_argument(\"--disable-gpu\")\n",
    "    chrome_opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_opts.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": str(download_dir.resolve()),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "    })\n",
    "    return webdriver.Chrome(options=chrome_opts)\n",
    "\n",
    "def handle_cookie_consent(driver):\n",
    "    try:\n",
    "        accept_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"ctl00_mainContentArea_disclaimerContent_yesButton\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", accept_button)\n",
    "        time.sleep(0.3)\n",
    "        accept_button.click()\n",
    "        print(\"Clicked 'Accept' button\")\n",
    "        time.sleep(0.5)\n",
    "        body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        ActionChains(driver).move_to_element_with_offset(body, 0, 0).click().perform()\n",
    "        print(\"Performed dummy click\")\n",
    "    except TimeoutException:\n",
    "        print(\"No cookie banner found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie error: {e}\")\n",
    "\n",
    "def click_disclosure_tab_with_retry(driver, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            disclosure_tab = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//a[@href=\"#tabDisclosureDocuments\"]'))\n",
    "            )\n",
    "            driver.execute_script('arguments[0].scrollIntoView({block: \"center\"});', disclosure_tab)\n",
    "            time.sleep(0.4)\n",
    "            driver.execute_script(\"arguments[0].click();\", disclosure_tab)\n",
    "            return\n",
    "        except StaleElementReferenceException:\n",
    "            print(f\"Attempt {attempt + 1}: Stale tab. Retrying...\")\n",
    "            time.sleep(1)\n",
    "    raise Exception(\"Could not click Disclosure tab\")\n",
    "\n",
    "def extract_tooltip_pdfs(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    results = []\n",
    "    for tooltip in soup.select(\"a.ihpQtipHelp.rtTip[help]\"):\n",
    "        help_html = tooltip.get(\"help\")\n",
    "        section_name = tooltip.get_text(strip=True)\n",
    "        if not help_html:\n",
    "            continue\n",
    "        inner_soup = BeautifulSoup(help_html, \"html.parser\")\n",
    "        for a in inner_soup.find_all(\"a\"):\n",
    "            href = a.get(\"href\")\n",
    "            doc_text = a.text.strip()\n",
    "            if href and href.endswith(\".pdf\"):\n",
    "                full_url = f\"https://emma.msrb.org{href}\" if not href.startswith(\"http\") else href\n",
    "                combined_name = f\"{section_name} - {doc_text}\"\n",
    "                results.append({\n",
    "                    \"document_name\": combined_name,\n",
    "                    \"pdf_url\": full_url\n",
    "                })\n",
    "    return results\n",
    "\n",
    "def download_via_requests(url, dest_path):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "        if r.status_code == 403:\n",
    "            return False\n",
    "        r.raise_for_status()\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[requests fail] {url} → {e}\")\n",
    "        return False\n",
    "\n",
    "def download_via_chrome(driver, url, dest_path):\n",
    "    TMP_DIR.mkdir(exist_ok=True)\n",
    "    for f in TMP_DIR.glob(\"*\"):\n",
    "        f.unlink()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(WAIT_TIME)\n",
    "        pdf_files = list(TMP_DIR.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            print(f\"[chrome fail] No PDF for {url}\")\n",
    "            return False\n",
    "        pdf_file = max(pdf_files, key=os.path.getctime)\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pdf_file.rename(dest_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[chrome error] {url} → {e}\")\n",
    "        return False\n",
    "\n",
    "def download_pdfs(final_df):\n",
    "    failed_downloads = []\n",
    "    driver = setup_browser(TMP_DIR)\n",
    "    try:\n",
    "        for credit, group in tqdm(final_df.groupby(\"CREDIT\"), desc=\"Universities\"):\n",
    "            folder = ROOT_DIR / slugify(credit)\n",
    "            for _, row in group.iterrows():\n",
    "                url = row[\"pdf_url\"]\n",
    "                name = slugify(row[\"document_name\"])\n",
    "                ext = Path(urlparse(url).path).suffix or \".pdf\"\n",
    "                target = folder / f\"{name}{ext}\"\n",
    "                if target.exists():\n",
    "                    continue\n",
    "                success = download_via_requests(url, target)\n",
    "                if not success:\n",
    "                    print(f\"[Chrome fallback] {url}\")\n",
    "                    success = download_via_chrome(driver, url, target)\n",
    "                if not success:\n",
    "                    failed_downloads.append({\n",
    "                        \"CREDIT\": credit,\n",
    "                        \"document_name\": row[\"document_name\"],\n",
    "                        \"pdf_url\": url\n",
    "                    })\n",
    "                time.sleep(SLEEP)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        for f in TMP_DIR.glob(\"*\"):\n",
    "            f.unlink()\n",
    "        TMP_DIR.rmdir()\n",
    "\n",
    "    if failed_downloads:\n",
    "        fail_df = pd.DataFrame(failed_downloads)\n",
    "        fail_df.to_csv(FAILED_LOG_PATH, index=False)\n",
    "        print(f\"\\nLogged {len(failed_downloads)} failed downloads to {FAILED_LOG_PATH}\")\n",
    "\n",
    "def filter_documents(df):\n",
    "    keywords = [\n",
    "        \"annual disclosure\",\n",
    "        \"financial statement\",\n",
    "        \"financial disclosure\",\n",
    "        \"audited financials\",\n",
    "        \"continuing disclosure\"\n",
    "    ]\n",
    "    return df[df['document_name'].str.lower().apply(\n",
    "        lambda x: any(keyword in x for keyword in keywords)\n",
    "    )]\n",
    "\n",
    "# ------------------ MAIN SCRAPING SECTION ------------------\n",
    "cookie_handled = False\n",
    "final_df = pd.DataFrame()\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://emma.msrb.org/\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for c in tqdm(list_cusip, desc=\"Scraping CUSIPs\"):\n",
    "    print(f\"\\nProcessing CUSIP: {c}\")\n",
    "    try:\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"quickSearchText\"))\n",
    "        )\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(c)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        if not cookie_handled:\n",
    "            handle_cookie_consent(driver)\n",
    "            cookie_handled = True\n",
    "\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//ul[contains(@class, \"ui-tabs-nav\")]'))\n",
    "        )\n",
    "\n",
    "        click_disclosure_tab_with_retry(driver)\n",
    "\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.ID, \"tabDisclosureDocuments\"))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            radio = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'input[name=\"Filter.SelectedPredefinedDateRange\"][value=\"All\"]'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", radio)\n",
    "            time.sleep(0.3)\n",
    "            if not radio.get_attribute(\"checked\"):\n",
    "                radio.click()\n",
    "        except Exception as e:\n",
    "            print(f\"Filter click failed: {e}\")\n",
    "\n",
    "        pdf_data = []\n",
    "        pdf_links = driver.find_elements(By.XPATH, '//a[contains(@ga-name, \"clickDisclosureDocuments\") and contains(@href, \".pdf\")]')\n",
    "\n",
    "        for i, link in enumerate(pdf_links):\n",
    "            try:\n",
    "                text = link.text.strip()\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if text and href and href.endswith(\".pdf\"):\n",
    "                    full_url = href if href.startswith(\"http\") else f\"https://emma.msrb.org{href}\"\n",
    "                    pdf_data.append({\"CUSIP\": c, \"document_name\": text, \"pdf_url\": full_url})\n",
    "            except StaleElementReferenceException:\n",
    "                print(f\"Skipping stale link {i} for CUSIP {c}\")\n",
    "\n",
    "        hidden_pdfs = extract_tooltip_pdfs(driver)\n",
    "        for pdf in hidden_pdfs:\n",
    "            pdf[\"CUSIP\"] = c\n",
    "            pdf_data.append(pdf)\n",
    "\n",
    "        if pdf_data:\n",
    "            temp_df = pd.DataFrame(pdf_data)\n",
    "            final_df = pd.concat([final_df, temp_df], ignore_index=True)\n",
    "            print(f\"Collected {len(pdf_data)} PDFs for {c}\")\n",
    "        else:\n",
    "            print(f\"No PDFs found for {c}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for CUSIP {c}: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nScraping took {end_time - start_time:.2f} seconds\")\n",
    "driver.quit()\n",
    "\n",
    "final_df = pd.merge(final_df, df_cusips, how='left', left_on='CUSIP', right_on='Cusip 8')\n",
    "final_df = final_df[['CUSIP', 'document_name', 'pdf_url', 'CREDIT']]\n",
    "final_df.to_csv(EXCEL_OUTPUT)\n",
    "\n",
    "final_df = filter_documents(final_df)\n",
    "download_pdfs(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63ad7ca9-78d3-495b-887c-c24a6d8c40f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142135b-2a15-433d-b1bd-e398705cdc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
